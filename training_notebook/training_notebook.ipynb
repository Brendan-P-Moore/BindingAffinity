{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b88d21f-8e58-426b-8d58-edd3a6f92285",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install biopython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d21bf8-107d-4620-9447-c1b9cbec8152",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install rdkit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2319dcd4-6d97-4eb3-9f30-0b03836e71e7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install chembl_webresource_client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33605dd5-b4af-4df9-993a-96e22923ca41",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install propy3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c938a08c-5097-4236-ad43-091421bcba0a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install peptides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d5ffee-da40-4e7b-962f-382aac18cde2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install autogluon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa40693-6568-45b2-befb-c42623ff166c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import Bio\n",
    "\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import rdFingerprintGenerator\n",
    "from rdkit.Chem.Draw import IPythonConsole\n",
    "from rdkit import DataStructs\n",
    "from rdkit.DataStructs import ConvertToNumpyArray\n",
    "import rdkit\n",
    "\n",
    "import requests as r\n",
    "from Bio import SeqIO\n",
    "from io import StringIO\n",
    "from chembl_webresource_client.new_client import new_client\n",
    "from multiprocessing import Pool, cpu_count\n",
    "\n",
    "import sklearn\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "import gc\n",
    "from functools import reduce\n",
    "import multiprocessing\n",
    "from tqdm import tqdm\n",
    "\n",
    "import peptides\n",
    "import propy\n",
    "from propy import PyPro\n",
    "\n",
    "import joblib\n",
    "from joblib import dump, load\n",
    "import os\n",
    "\n",
    "from autogluon.features.generators import AutoMLPipelineFeatureGenerator\n",
    "from autogluon.tabular import FeatureMetadata\n",
    "from autogluon.tabular import TabularDataset, TabularPredictor\n",
    "\n",
    "import time\n",
    "from requests.exceptions import Timeout"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21834bc1-74d3-4450-8e4d-e218820b72c3",
   "metadata": {},
   "source": [
    "# Feature Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d87d08c-d2a0-4368-93e0-8bd9b095e887",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert protein uniprot ids to sequences for feature generation.\n",
    "def uniprot_to_fasta(df):\n",
    "    uniprot_ids = df[\"target_id\"].drop_duplicates().tolist()\n",
    "    baseUrl = \"https://rest.uniprot.org/uniprotkb/\"\n",
    "    seq = []\n",
    "    for id in tqdm(uniprot_ids, desc=\"Converting uniprot IDs to protein sequences\"):\n",
    "        currentUrl = baseUrl + id + \".fasta\"\n",
    "        response = r.get(currentUrl)\n",
    "        Seq = StringIO(response.text)\n",
    "        pSeq = list(SeqIO.parse(Seq, \"fasta\"))\n",
    "        if not pSeq:  # Check if pSeq is empty\n",
    "            seq.append(\"\")  # Append an empty string if no sequence was found\n",
    "            print(f\"Warning: No sequence found for Uniprot ID {id}.\")\n",
    "        else:\n",
    "            seq_records = []\n",
    "            for seq_record in pSeq:\n",
    "                seq_records.append(str(seq_record.seq))\n",
    "                break\n",
    "            seq.append(seq_records[0])\n",
    "    df[\"sequence\"] = df[\"target_id\"].map(dict(zip(uniprot_ids, seq)))\n",
    "    df.dropna(inplace=True)\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "# for converting chembl id to smiles strings in batches, avoids timeout errors using the chembl web api.\n",
    "def process_batch(batch):\n",
    "    molecules = new_client.molecule.filter(molecule_chembl_id__in=batch).only(\n",
    "        \"molecule_chembl_id\", \"molecule_structures\"\n",
    "    )\n",
    "    smiles = []\n",
    "    chembl_ids_with_smiles = []\n",
    "    for molecule in molecules:\n",
    "        if (\n",
    "            molecule[\"molecule_structures\"] is not None\n",
    "            and \"canonical_smiles\" in molecule[\"molecule_structures\"]\n",
    "        ):\n",
    "            smiles.append(molecule[\"molecule_structures\"][\"canonical_smiles\"])\n",
    "            chembl_ids_with_smiles.append(molecule[\"molecule_chembl_id\"])\n",
    "    return dict(zip(chembl_ids_with_smiles, smiles))\n",
    "\n",
    "\n",
    "# convert compound chembl ids to smiles strings for molecular feature generations, multiprocessing to speed up process.\n",
    "def chembl_to_smiles(df):\n",
    "    chembl_ids = df[\"compound_id\"].unique().tolist()\n",
    "    batch_size = 500\n",
    "    chembl_id_batches = [\n",
    "        chembl_ids[i : i + batch_size] for i in range(0, len(chembl_ids), batch_size)\n",
    "    ]\n",
    "    num_processes = cpu_count()\n",
    "    with Pool(num_processes) as pool:\n",
    "        results = []\n",
    "        for result in tqdm(\n",
    "            pool.imap_unordered(process_batch, chembl_id_batches),\n",
    "            total=len(chembl_id_batches),\n",
    "        ):\n",
    "            results.append(result)\n",
    "    merged_results = {}\n",
    "    for d in results:\n",
    "        merged_results.update(d)\n",
    "    df[\"compound_smiles\"] = df[\"compound_id\"].map(merged_results)\n",
    "    df.dropna(inplace=True)\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "# generate a molecule from a smiles string.\n",
    "def smi_to_mol(smi):\n",
    "    return Chem.MolFromSmiles(smi)\n",
    "\n",
    "\n",
    "# multiprocess the smiles to molecule conversion to speed up the process.\n",
    "def smiles_to_rdkit_mol(df):\n",
    "    with Pool(cpu_count()) as p:\n",
    "        mols = p.map(smi_to_mol, df[\"compound_smiles\"])\n",
    "    mols = [mol for mol in mols if mol is not None]\n",
    "    return mols\n",
    "\n",
    "\n",
    "# generate molecular fingerprints (morgan300, toptorsion300) from smiles strings.\n",
    "def fingerprint_generation(\n",
    "    df, ncounts\n",
    "):  # returns a df with molecular fingerprints, ncounts is the number of count vectors.\n",
    "    mols = smiles_to_rdkit_mol(df)\n",
    "    # fingerprint generation\n",
    "    mfpgen = rdFingerprintGenerator.GetMorganGenerator(\n",
    "        radius=2, fpSize=ncounts\n",
    "    )  # morgan300 fingerprint\n",
    "    ttgen = rdFingerprintGenerator.GetTopologicalTorsionGenerator(\n",
    "        fpSize=ncounts\n",
    "    )  # TopologicalTorsion300 Fingerprint\n",
    "    mfp_matrix = np.zeros((len(mols), ncounts), dtype=int)\n",
    "    tt_matrix = np.zeros((len(mols), ncounts), dtype=int)\n",
    "    mfp_fingerprints = [\n",
    "        mfpgen.GetCountFingerprint(mol)\n",
    "        for mol in tqdm(mols, desc=\"Generating Morgan fingerprints\", unit=\" molecule\")\n",
    "    ]\n",
    "    tt_fingerprints = [\n",
    "        ttgen.GetCountFingerprint(mol)\n",
    "        for mol in tqdm(\n",
    "            mols, desc=\"Generating Topological Torsion fingerprints\", unit=\" molecule\"\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    for i, fingerprint in enumerate(mfp_fingerprints):\n",
    "        ConvertToNumpyArray(fingerprint, mfp_matrix[i])\n",
    "    mfp_df = pd.DataFrame(\n",
    "        mfp_matrix, columns=[f\"morg_count_fp_{i}\" for i in range(ncounts)]\n",
    "    ).astype(\"int8\")\n",
    "\n",
    "    for i, fingerprint in enumerate(tt_fingerprints):\n",
    "        ConvertToNumpyArray(fingerprint, tt_matrix[i])\n",
    "    tt_df = pd.DataFrame(\n",
    "        tt_matrix, columns=[f\"tt_fp_{i}\" for i in range(ncounts)]\n",
    "    ).astype(\"int8\")\n",
    "\n",
    "    merged_df = pd.concat([df, mfp_df, tt_df], axis=1)\n",
    "    return merged_df\n",
    "\n",
    "\n",
    "# for calculating propy protein descriptors.\n",
    "def calculate_descriptors(unique_seqs, calculation_function):\n",
    "    unique_descriptors = {}\n",
    "    for seq in tqdm(unique_seqs, desc=\"Calculating propy package descriptors\"):\n",
    "        unique_descriptors[seq] = calculation_function(seq)\n",
    "    descriptor_names = set().union(*unique_descriptors.values())\n",
    "    descriptor_dict = {}\n",
    "    for name in descriptor_names:\n",
    "        descriptor_dict[name] = {}\n",
    "        for seq, desc in unique_descriptors.items():\n",
    "            descriptor_dict[name][seq] = desc.get(name, None)\n",
    "    return descriptor_dict\n",
    "\n",
    "\n",
    "# function to generate all required protein descriptors for model training.\n",
    "def protein_feature_generation(df):\n",
    "    unique_seqs = set(df[\"sequence\"])\n",
    "\n",
    "    # Calculate descriptors using peptides library with full code, different syntax required.\n",
    "    unique_descriptors = {}\n",
    "    for seq in tqdm(unique_seqs, desc=\"Calculating all peptide package descriptors\"):\n",
    "        unique_descriptors[seq] = peptides.Peptide(seq).descriptors()\n",
    "\n",
    "    descriptor_names = set().union(*unique_descriptors.values())\n",
    "    descriptor_dict = {}\n",
    "    for name in descriptor_names:\n",
    "        descriptor_dict[name] = {}\n",
    "        for seq, desc in unique_descriptors.items():\n",
    "            descriptor_dict[name][seq] = desc.get(name, None)\n",
    "    peptide_df = pd.DataFrame.from_dict(descriptor_dict)\n",
    "    peptide_df[\"sequence\"] = peptide_df.index\n",
    "\n",
    "    # Calculate descriptors using propy.CTD library\n",
    "    unique_descriptors = calculate_descriptors(unique_seqs, propy.CTD.CalculateCTD)\n",
    "\n",
    "    ctd_df = pd.DataFrame.from_dict(unique_descriptors)\n",
    "    ctd_df[\"sequence\"] = ctd_df.index\n",
    "\n",
    "    # Calculate descriptors using propy.Autocorrelation library\n",
    "    unique_descriptors = calculate_descriptors(\n",
    "        unique_seqs, propy.Autocorrelation.CalculateGearyAutoTotal\n",
    "    )\n",
    "    autocorr_df = pd.DataFrame.from_dict(unique_descriptors)\n",
    "    autocorr_df[\"sequence\"] = autocorr_df.index\n",
    "\n",
    "    # Calculate descriptors using propy.AAComposition library\n",
    "    unique_descriptors = calculate_descriptors(\n",
    "        unique_seqs, propy.AAComposition.CalculateAAComposition\n",
    "    )\n",
    "    aa_df = pd.DataFrame.from_dict(unique_descriptors)\n",
    "    aa_df[\"sequence\"] = aa_df.index\n",
    "\n",
    "    # Merge all dataframes into one\n",
    "    df = pd.merge(df, peptide_df, on=\"sequence\")\n",
    "    df = pd.merge(df, ctd_df, on=\"sequence\")\n",
    "    df = pd.merge(df, autocorr_df, on=\"sequence\")\n",
    "    df = pd.merge(df, aa_df, on=\"sequence\")\n",
    "\n",
    "    df[\"sequence_length\"] = df[\"sequence\"].str.len()\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91240b82-9c4d-42e4-9e89-061f48c8bc5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load csv\n",
    "train = pd.read_csv('train_dataframe.csv')\n",
    "# convert uniprot to sequence\n",
    "train = uniprot_to_fasta(train)\n",
    "# convert chembl to smiles\n",
    "train = chembl_to_smiles(train)\n",
    "# generate molecular fingerprints\n",
    "train = fingerprint_generation(train,300)\n",
    "# generate protein features\n",
    "train =protein_feature_generation(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13923c8f-ad7d-4d93-88a0-b4133fd446fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_autogluon(df):\n",
    "    exclude_autogluon= ['compound_id','target_id','group','sequence','compound_smiles']\n",
    "    features_autogluon = [c for c in df.columns if c not in exclude_autogluon]\n",
    "    os.system('mkdir autogluon_models_check')\n",
    "    autogluon_folds = 10\n",
    "    gkf = GroupKFold(n_splits=autogluon_folds)\n",
    "    oof = np.zeros(len(df))\n",
    "    for fold,(train_idx, valid_idx) in enumerate(gkf.split(\n",
    "                df, df.pchembl_value,df.group)):\n",
    "        print('#'*25)\n",
    "        print('### Fold',fold+1)\n",
    "        print('### Train size',len(train_idx),'Valid size',len(valid_idx))\n",
    "        print('#'*25)\n",
    "        X_train = df.loc[train_idx, features_autogluon]\n",
    "        y_train = df.loc[train_idx,'pchembl_value']\n",
    "        X_valid = df.loc[valid_idx, features_autogluon]\n",
    "        y_valid = df.loc[valid_idx,'pchembl_value']\n",
    "        predictor = TabularPredictor(label = 'pchembl_value', problem_type='regression', eval_metric='rmse', path=f'./autogluon_models_check/fold{fold}')\n",
    "        predictor.fit(train_data = X_train,tuning_data = X_valid, time_limit=5*60*60, auto_stack = False,\n",
    "                      _save_bag_folds = False,hyperparameters='light', keep_only_best= True,excluded_model_types=['CAT','RF','XT','FASTAI'], save_space=True)\n",
    "        predictor.leaderboard(X_valid)\n",
    "        model = TabularPredictor.load(f'./autogluon_models_check/fold{fold}/')\n",
    "        oof_preds = model.predict(X_valid)\n",
    "        oof[valid_idx] = oof_preds\n",
    "    rmse = mean_squared_error(df.pchembl_value, oof,squared=False)\n",
    "    print('OVERALL rmse =',rmse,'\\n')\n",
    "    df['autogluon_oof'] = oof\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12eeee1e-1a21-41b4-81b9-6b5b91219226",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train_autogluon(train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
